# From Perceptrons to Deep Networks
### Perceptrons: early Deep Learning algorithms
1. basic neural network building block
2. for supervised learning
3. finishing jobs that mapping samples to labels
#### Procedure:
1. combine the inputs and weights with weighted sum transfer function:

    ![weighted sum](https://uploads.toptal.io/blog/image/348/toptal-blog-image-1395939315757.54.56_PM.png)
2. generate the activated result using a specific activation function, here take the simplest one as an example:

    ![activate](https://uploads.toptal.io/blog/image/349/toptal-blog-image-1395939338622.55.31_PM.png)

#### Training:
1. feed it with multiple input samples and calculate the outputs.
2. weights in it are adjusted in such a way so as to minimise the output error, defined as the the difference between generated output and the actual target. There are many output error functions, like MSE(mean square error).

#### Advance: feedfoward neural network
It consist of multilayer perceptrons. The first layer of which is known as the input layer and the last layer of which is known as the output layer. The rest of which are hidden layers.

It is always used to approximate a function that relate a finite dimention with another dimention. The more complex the function to approximate is, the larger the network is.

The function approximate could be non-linear only if activation functions in some of the layers are non-linear.

About how to minimise the error: **_gradient descent._**

    Problems with Large Networks:
    1. vanishing gradients: as we add more and more hidden layers, the backpropregation become less and less useful in passing information to lower layers.
    2. overfiting: the network can be trained to be so complex that it can suits the training data well but perform badly in the testing data.
___
### Some Deep Learning Algorithm to Address these Issues
#### 1. Autoencoders
An autoencoder is typically a feedforward neural network which aims to learn a compressed, distributed representation(encoding) of the input data.

![Autoencoders](https://uploads.toptal.io/blog/image/340/toptal-blog-image-1395769623098.png)

Conceptually, the network is trained to _“recreate”_ the input

#### 2. Restricted Boltzmann Machines
A RBMs consist of visible units and hidden units.

![RBM](https://uploads.toptal.io/blog/image/351/toptal-blog-image-1395942212600.png)

Specially, the connections between units are undirected(both visible-to-hidden and hidden-to-visible) and fully connected

##### A newly introduced unsupervised training algorithm: Contrastive Divergence
The single-step contrastive divergence algorithm (CD-1) works like this:
1. Positive phase:
An input sample v is clamped to the input layer.
v is propagated to the hidden layer in a similar manner to the feedforward networks. The result of the hidden layer activations is h.
2. Negative phase:
Propagate h back to the visible layer with result v’ (the connections between the visible and hidden layers are undirected and thus allow movement in both directions).
Propagate the new v’ back to the hidden layer with activations result h’.
3. Weight update:

![CD Weight update](https://uploads.toptal.io/blog/image/350/toptal-blog-image-1395939562020.58.50_PM.png)

Where a is the learning rate and v, v’, h, h’, and w are vectors.

The intuition behind the algorithm is that the positive phase is aiming at the internal representation of the real world(h given v) while the negative phase tries to recreate the date based on this internal representation(h' given v').

The main goal is for the generated data to be as closed as possible to the real world, which can be inferred int the formula above.

#### 3. Stacked Autoencoders
Stacked Autoencoders is a neural network with many stacked autoencoders.

##### Training:
The hidden layer of autoencoder t acts as an input layer to autoencoder t + 1. The input layer of the first autoencoder is the input layer for the whole network. The greedy layer-wise training procedure works like this:

![Stacked Autoencoders Training](https://uploads.toptal.io/blog/image/335/toptal-blog-image-1395721542588.png)

1. The input data are clamped to the input units and then propagate to the hidden layer of the first autoencoders.
2. Add an output layer that receive the propagation from the hidden layer, which is followed by calculate the difference between output and input.
3. Backpropagation is performed to adjust weights.
4. Remove the output layer.
5. Repeat step 2 to 4 for all the layers

#### 4. Deep Belief Networks
