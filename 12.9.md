# Optimizers
## SGD
#### 1.BGD
> θ=θ−η∇θJ(θ)\

每个循环算梯度都要把**整个训练集**拿来算，标准的梯度下降
```
for i in range(nb_epochs):
    params_grad = evaluate_gradient(loss_function, data, params)
    params = params - learning_rate * params_grad
```
### 2.SGD
> θ=θ−η∇θJ(x(i),y(i);θ)

每次只做一次参数更新，只计算**一个样本**
```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for example in data:
        params_grad = evaluate_gradient(loss_function, example, params)
        params = params - learning_rate * params_grad
```
_question:感觉反而会慢一点，迭代次数多了，虽然结果还是一个data的大小_
### 3.MGD
>θ=θ−η∇θJ(x(i:i+n),y(i:i+n);θ)

把训练集分成batch，用每个**batch分别分开做参数更新**
```python
for i in range(nb_epochs):
    np.random.shuffle(data)
    for batch in get_batches(data, batch_size=50):
        params_grad = evaluate_gradient(loss_function, batch, params)
        params = params - learning_rate * params_grad
```
>网上表示第一种标准的BGD太慢，因为更新一次参数需要用到整个训练集
>第二种SGD是比较快的，一步一步来，在很快速的震荡中下降
>第三种MGD结合了前两种的优点

#### SGD的弱点：
1. 选好learning rate很难
2. 容易局部最优
3. 在梯度较大的地方仍然下降不快，就是说，**learning rate在整个过程没变**

## Momentum
>mt = μ∗mt−1 + η∇θJ(θ)\
θt=θt−1−mt
 
脑洞比较大，用到了物理里面动量的想法

简单来说就是：**用一个量存储了过去的梯度的平均值，然后用这个梯度的平均值作为当前跟新参数的梯度**
#### **特点**
1. 参数调整初期，若紧邻的前后两次下降方向一样的话，这种方法一定程度上会加速下降
2. 参数调整中后期，因为动量的积累，参数值在局部最小震荡时能自动跳出
3. 梯度下降中紧邻的两次下降方向不相同时，能有效减慢速度。
4. **总之，能抑制震荡，加快收敛**

还有一些还没写，明天写。